{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_metodo(modelo, X_train, y_train, X_test, y_test, mostrar=False):\n",
    "    \n",
    "    # escalamos en la funcion porque a veces no funciona, por las dudas\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)  # escalamos los datos de entrenamiento\n",
    "    X_test = scaler.fit_transform(X_test)  # escalamos los datos de prueba\n",
    "    \n",
    "    modelo.fit(X_train, y_train) \n",
    "    \n",
    "    # Predecimos con los datos de prueba\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    y_proba = modelo.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "    # calcula las metricas que vamos a reprotar\n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred) \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba) \n",
    "    \n",
    "    # estas son las que vamos a usar para comparar:\n",
    "    score_auc = auc(fpr, tpr)  \n",
    "    accuracy = accuracy_score(y_test, y_pred)  \n",
    "    mse = mean_squared_error(y_test, y_pred)  \n",
    "    \n",
    "    # si mostrar es true, muestra la curva ROC\n",
    "    if mostrar:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {score_auc:.2f}\")  # grafica la curva ROC\n",
    "        plt.plot([0, 1], [0, 1], 'r--')  # linea de no habilidad\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # devuelve las metricas en un diccionario\n",
    "    return { 'Matriz de Confusión': matriz_confusion, 'AUC': score_auc,\n",
    "        'Accuracy': accuracy, 'MSE': mse\n",
    "    }\n",
    "\n",
    "def cross_validation(modelo, k, X, y):\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)  # configuracion del k-fold\n",
    "    resultados = {\n",
    "        'Accuracy': [],\n",
    "        'AUC': [],\n",
    "        'MSE': []\n",
    "    }\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):  # divide los datos en k folds\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        resultado = evalua_metodo(modelo, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        # agrega los resultados de cada fold a las listas\n",
    "        resultados['AUC'].append(resultado['AUC'])\n",
    "        resultados['Accuracy'].append(resultado['Accuracy'])\n",
    "        resultados['MSE'].append(resultado['MSE'])\n",
    "\n",
    "    # calcula el promedio de cv de las medidas\n",
    "    promedios = {key: np.mean(val) for key, val in resultados.items()}\n",
    "    \n",
    "    return promedios\n",
    "\n",
    "def evalua_config(modelo_base, configs, k, X, y):\n",
    "    resultados = []\n",
    "    for config in configs:\n",
    "        modelo = clone(modelo_base)\n",
    "        modelo.set_params(**config)\n",
    "        resultado_cv = cross_validation(modelo, k, X, y)\n",
    "        resultados.append({\n",
    "            'configuracion': config,\n",
    "            'error_promedio': resultado_cv['MSE']\n",
    "        })\n",
    "    \n",
    "    mejor_resultado = min(resultados, key=lambda x: x['error_promedio']) # agarra la config con menor mse\n",
    "    return mejor_resultado\n",
    "\n",
    "def evalua_multiples_metodos(X, y):\n",
    "    # divide datos en entrenamiento, validacion y prueba\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "    \n",
    "    # configs para regresion logistica barriendo lambdas\n",
    "    configuraciones_lr = [{'C': 1/l, 'penalty': 'l2', 'max_iter': 200} for l in np.logspace(-5, 5, 11)]\n",
    "    \n",
    "    # Buscamos el mejor lambda\n",
    "    mejor_config_lr = evalua_config(LogisticRegression(solver='liblinear'), configuraciones_lr, 5, X_train, y_train)\n",
    "    modelo_lr = LogisticRegression(**mejor_config_lr['configuracion'], solver='liblinear')\n",
    "    \n",
    "    # evalua modelos\n",
    "    resultados = []\n",
    "    modelos = {\n",
    "        'Logistic Regression': modelo_lr,\n",
    "        'LDA': LDA(),\n",
    "        'KNN (K=3)': KNeighborsClassifier(n_neighbors=3)\n",
    "    }\n",
    "    \n",
    "    for nombre, modelo in modelos.items():\n",
    "        # evaluacion final con configuracion optima\n",
    "        metricas = evalua_metodo(modelo, X_train_val, y_train_val, X_test, y_test)\n",
    "        resultados.append({\n",
    "            'Modelo': nombre,\n",
    "            # esto para que sea mas legible el output:\n",
    "            'Configuración': \", \".join([f\"{k}: {v}\" for k, v in modelo.get_params().items() if k in ['C', 'penalty', 'n_neighbors']]),\n",
    "            'AUC': metricas['AUC'],\n",
    "            'Accuracy': metricas['Accuracy'],\n",
    "            'MSE': metricas['MSE']\n",
    "        })\n",
    "    \n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    # formatea las columnas numericas\n",
    "    df_resultados[['AUC', 'Accuracy', 'MSE']] = df_resultados[['AUC', 'Accuracy', 'MSE']].applymap(lambda x: f\"{x:.3f}\")\n",
    "    return df_resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: {'Matriz de Confusión': array([[ 56,   7],\n",
      "       [  2, 106]]), 'AUC': 0.9907407407407407, 'Accuracy': 0.9473684210526315, 'MSE': 0.05263157894736842}\n",
      "KNN CV: {'Accuracy': 0.9472442167365317, 'AUC': 0.9877519236729324, 'MSE': 0.0527557832634684}\n",
      "KNN: {'Matriz de Confusión': array([[ 59,   4],\n",
      "       [  1, 107]]), 'AUC': 0.9805261610817166, 'Accuracy': 0.9707602339181286, 'MSE': 0.029239766081871343}\n",
      "KNN CV: {'Accuracy': 0.9507529886663562, 'AUC': 0.9864858906508879, 'MSE': 0.04924701133364384}\n",
      "Logistic Regression: {'Matriz de Confusión': array([[ 60,   3],\n",
      "       [  1, 107]]), 'AUC': 0.9979423868312757, 'Accuracy': 0.9766081871345029, 'MSE': 0.023391812865497075}\n",
      "Logistic Regression CV: {'Accuracy': 0.9753920198726906, 'AUC': 0.9954850276788912, 'MSE': 0.02460798012730942}\n",
      "Mejor configuración para Logistic Regression: {'configuracion': {'C': 1.0, 'penalty': 'l2', 'max_iter': 500}, 'error_promedio': 0.02460798012730942}\n",
      "                Modelo        Configuración    AUC Accuracy    MSE\n",
      "0  Logistic Regression  C: 0.1, penalty: l2  0.999    0.982  0.018\n",
      "1                  LDA                       0.990    0.939  0.061\n",
      "2            KNN (K=3)       n_neighbors: 3  0.972    0.956  0.044\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instanciar los modelos\n",
    "modelo_lda = LDA()\n",
    "modelo_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "modelo_logistic = LogisticRegression()\n",
    "\n",
    "# Evaluar LDA\n",
    "print(\"KNN:\", evalua_metodo(modelo_lda, X_train, y_train, X_test, y_test))\n",
    "print(\"KNN CV:\", cross_validation(modelo_lda, 5, X, y))\n",
    "\n",
    "# Evaluar KNN\n",
    "print(\"KNN:\", evalua_metodo(modelo_knn, X_train, y_train, X_test, y_test))\n",
    "print(\"KNN CV:\", cross_validation(modelo_knn, 5, X, y))\n",
    "\n",
    "# Evaluar Logistic Regression\n",
    "print(\"Logistic Regression:\", evalua_metodo(modelo_logistic, X_train, y_train, X_test, y_test))\n",
    "print(\"Logistic Regression CV:\", cross_validation(modelo_logistic, 5, X, y))\n",
    "configuraciones_lr = [{'C': 1/l, 'penalty': 'l2', 'max_iter': 500} for l in np.logspace(-5,5,11)]\n",
    "mejor_config_lr = evalua_config(LogisticRegression(), configuraciones_lr, 5, X, y)\n",
    "print(\"Mejor configuración para Logistic Regression:\", mejor_config_lr)\n",
    "\n",
    "\n",
    "# Uso de la función\n",
    "df_resultados = evalua_multiples_metodos(X, y)\n",
    "print(df_resultados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
